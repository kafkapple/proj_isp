defaults:
  - _self_
  - override hydra/sweeper: basic  # basic sweeper 사용

general:
  outputs: outputs/${now:%Y%m%d_%H%M%S}  # Default outputs path
  random_state: 42
# Experiment variations
hydra:
  # multirun 출력 디렉토리 설정
  sweep:
    dir: ${general.outputs}
    subdir: ${model.use_rag}
  
  sweeper:
    # basic sweeper 설정
    params:
      model.use_rag: true,false
  
  # 기본 실행 설정
  run:
    dir: ${general.outputs}/${model.use_rag}
  
  # 멀티런 설정
  mode: MULTIRUN
  output_subdir: log  # Save hydra logs to outputs/[date]/[time]/log
  
debug:
  show_prompt: false
  show_generation: false #true
  show_retrieval: true
  show_full_prompt: false  # 전체 프롬프트 출력 여부

data:
  name: "isear"  # Dataset name (isear, custom, etc.)
  csv_file: "isear.csv"  # File name to save
  n_samples: 100 #-1  # Number of samples to use (-1 for all)
  train_size: 0.8
  val_size: 0.2
  column_mapping:
    text: "SIT"
    emotion: "EMOT"
  datasets:
    isear:
      urls:
        - "https://raw.githubusercontent.com/sinmaniphel/py_isear_dataset/master/isear.csv"
      required_columns: ["SIT", "EMOT"]
      separator: "|"
      emotions:
        # 1-7 in order of emotion classes
        classes: ["joy", "fear", "anger", "sadness", "disgust", "shame", "guilt"]
# Remove the top-level emotions section and reference dataset emotions
emotions: ${data.datasets.${data.name}.emotions}

model:
  use_rag: true  # Whether to use RAG
  provider: "lmstudio" #"openai"
  openai:
    chat_model_name: "gpt-3.5-turbo"
    embedding_model: "text-embedding-3-small"
  embeddings:
    default_model: "BAAI/bge-m3"
    fallback_model: "all-MiniLM-L6-v2"
  lmstudio:
    base_url: "http://localhost:1234/v1"
    api_key: "lm-studio"
    embedding_model: "text-embedding-bge-m3"  # Default value when auto-detection fails
  retriever:
    k: 3  # Number of documents to return (1-20 recommended)
    score_threshold: 1.5  # L2 distance threshold (1.0-2.0 recommended)
    fetch_k: 10  # Initial candidate fetch count (3-4 times k recommended)
    search_type: "similarity"  # Search method ("similarity" or "mmr")
    # MMR (Maximum Marginal Relevance) related settings
    mmr:
      lambda_mult: 0.5  # Diversity vs similarity trade-off (0: max diversity, 1: max similarity)
    # Performance related settings
    nprobe: 10  # Number of clusters to search (higher means higher accuracy but slower speed, 4-64 recommended)
    ef_search: 40  # HNSW graph search range (higher means higher accuracy but slower speed, 20-200 recommended)
  templates:
    base: |
      You are an expert in emotion analysis. Your task is to classify the emotion as EXACTLY ONE of these words: ${data.datasets.${data.name}.emotions.classes}

      CRITICAL INSTRUCTIONS:
      1. You MUST return ONLY ONE WORD from this list: ${data.datasets.${data.name}.emotions.classes}
      2. DO NOT use any other words or emotions
      3. DO NOT add any explanation or additional text
      4. DO NOT modify or combine the emotion words
      5. Your response must be EXACTLY ONE of these words, nothing else
      6. If you're unsure, choose the closest matching emotion from the list

      Text to classify: {input}
      Emotion (select ONE from the list above):

    rag: |
      You are an expert in emotion analysis. Your task is to classify the emotion as EXACTLY ONE of these words: ${data.datasets.${data.name}.emotions.classes}

      CRITICAL INSTRUCTIONS:
      1. You MUST return ONLY ONE WORD from this list: ${data.datasets.${data.name}.emotions.classes}
      2. DO NOT use any other words or emotions
      3. DO NOT add any explanation or additional text
      4. DO NOT modify or combine the emotion words
      5. Your response must be EXACTLY ONE of these words, nothing else
      6. If you're unsure, choose the closest matching emotion from the list

      Here are some similar examples for reference:
      {examples}

      Text to classify: {input}
      Emotion (select ONE from the list above):

analysis:
  embedding:
    method: "dec"  # "traditional" or "dec"
    traditional:
      reducer:
        type: "umap"  # "pca" or "umap"
        pca:
          n_components: 2
        umap:
          n_neighbors: 15
          min_dist: 0.1
          n_components: 2
      clustering:
        type: "gmm"  # "kmeans" or "gmm"
        k_range: [2, 3, 4, 5, 6, 7, 8, 9, 10]
        kmeans:
          n_init: 10
          random_state: 42
        gmm:
          covariance_type: "full"  # "full", "tied", "diag", "spherical"
          random_state: 42
        best_k_criterion:
          type: "combined"  # "intrinsic", "extrinsic", or "combined"
          metric: "average"
    dec:
      pretrain_epochs: 10
      clustering_epochs: 10
      batch_size: 256
      update_interval: 140
      tol: 0.001
      learning_rate: 0.001
      hidden_dims: [500, 500, 2000, 10]
    visualization:
      scatter:
        cmap: "Set2"
        alpha: 0.7
      heatmap:
        cmap: "RdYlBu_r"
        fmt: ".2f"
    metrics:
      intrinsic: ["silhouette", "calinski_harabasz", "davies_bouldin"]
      extrinsic: ["adjusted_rand", "normalized_mutual_info"]

