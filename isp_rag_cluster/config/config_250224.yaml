defaults:
  - _self_
  - override hydra/sweeper: basic  # basic sweeper 사용

general:
  outputs: outputs/${now:%Y%m%d_%H%M%S}  # Default outputs path
  random_state: 42
# Experiment variations
hydra:
  # multirun 출력 디렉토리 설정
  sweep:
    dir: ${general.outputs}
    subdir: ${model.use_rag}
  
  sweeper:
    # basic sweeper 설정
    params:
      model.use_rag: true,false
  
  # 기본 실행 설정
  run:
    dir: ${general.outputs}/${model.use_rag}
  
  # 멀티런 설정
  mode: RUN #MULTIRUN
  output_subdir: log  # Save hydra logs to outputs/[date]/[time]/log
  
debug:
  show_prompt: true      # 프롬프트 템플릿 자체를 보여줌
  show_generation: true  # LLM의 생성 결과와 처리 과정을 보여줌
  show_retrieval: true   # RAG 사용 시 검색된 문서들을 보여줌
  show_full_prompt: true # 실제 LLM에 전달되는 완성된 프롬프트를 보여줌

data:
  name: "isear"  # Dataset name (isear, custom, etc.)
  csv_file: "isear.csv"  # File name to save
  n_samples: 100 # Number of samples to use (-1 for all)
  train_size: 0.8
  val_size: 0.2
  column_mapping:
    text: "SIT"
    emotion: "EMOT"
  datasets:
    isear:
      urls:
        - "https://raw.githubusercontent.com/sinmaniphel/py_isear_dataset/master/isear.csv"
      required_columns: ["SIT", "EMOT"]
      separator: "|"
      emotions:
        # 1-7 in order of emotion classes
        classes: ["joy", "fear", "anger", "sadness", "disgust", "shame", "guilt"]
# Remove the top-level emotions section and reference dataset emotions
emotions: ${data.datasets.${data.name}.emotions}

model:
  use_rag: true  # Whether to use RAG
  provider: "openai" #"lmstudio" #"openai"
  openai:
    chat_model_name: "gpt-3.5-turbo"
    embedding_model: "text-embedding-ada-002"
    temperature: 0.0  # 추가: temperature 설정
  embeddings:
    default_model: "BAAI/bge-m3"
    fallback_model: "all-MiniLM-L6-v2"
  lmstudio:
    base_url: "http://localhost:1234/v1"
    api_key: "lm-studio"
    embedding_model: "text-embedding-bge-m3"  # Default value when auto-detection fails
    temperature: 0.0
  retriever:
    k: 3  # Number of documents to retrieve
    fetch_k: 5  # Number of documents to fetch before filtering
    score_threshold: 0.7  # Similarity score threshold
    search_type: "similarity"  # or "mmr"
    mmr:
      lambda_mult: 0.5
  templates:
    base: |
      You are an expert in emotion analysis. Your task is to classify the emotion in the given text into EXACTLY ONE of these words: ${data.datasets.${data.name}.emotions.classes}
      
      CRITICAL INSTRUCTIONS:
      1. You MUST return ONLY ONE WORD from the list.
      2. Do NOT add any explanation or additional text.
      3. In cases where multiple emotions are present, select the most dominant one.
      4. (Optional) The provided similarity scores in the examples are for reference only.

      Classify the following text:
      Text: {input}
      Emotion :

    rag: |
      You are an expert in emotion analysis. Your task is to classify the emotion in the given text into EXACTLY ONE of these words: ${data.datasets.${data.name}.emotions.classes}

      CRITICAL INSTRUCTIONS:
      1. You MUST return ONLY ONE WORD from the list.
      2. Do NOT add any explanation or additional text.
      3. In cases where multiple emotions are present, select the most dominant one.
      4. (Optional) The provided similarity scores in the examples are for reference only.

      EXAMPLES:
      {context}

      Based on these examples, classify the following text:
      Text: {input}
      Emotion :

analysis:
  embedding:
    method: "dec"  # "traditional" or "dec"
    traditional:
      reducer:
        type: "umap"  # "pca" or "umap"
        pca:
          n_components: 2
        umap:
          n_neighbors: 15
          min_dist: 0.1
          n_components: 2
      clustering:
        type: "gmm"  # "kmeans" or "gmm"
        k_range: [2, 3, 4, 5, 6, 7, 8, 9, 10]
        kmeans:
          n_init: 10
          random_state: 42
        gmm:
          covariance_type: "full"  # "full", "tied", "diag", "spherical"
          random_state: 42
        best_k_criterion:
          type: "combined"  # "intrinsic", "extrinsic", or "combined"
          metric: "average"
    dec:
      pretrain_epochs: 10
      clustering_epochs: 10
      batch_size: 256
      update_interval: 140
      tol: 0.001
      learning_rate: 0.001
      hidden_dims: [500, 500, 2000, 10]
    visualization:
      scatter:
        cmap: "Set2"
        alpha: 0.7
      heatmap:
        cmap: "RdYlBu_r"
        fmt: ".2f"
    metrics:
      intrinsic: ["silhouette", "calinski_harabasz", "davies_bouldin"]
      extrinsic: ["adjusted_rand", "normalized_mutual_info"]

