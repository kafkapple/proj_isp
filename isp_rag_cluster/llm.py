from dotenv import load_dotenv, find_dotenv
import os
import pandas as pd
import json
import re
from ollama import chat, ChatResponse
from anthropic import Anthropic  # Anthropic íŒ¨í‚¤ì§€ ì¶”ê°€
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
load_dotenv()
from pathlib import Path
import hydra
from tqdm import tqdm
from openai import OpenAI
from omegaconf import OmegaConf
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from rag import EmotionRAG
import logging
import sys
from typing import Dict, Any
from langchain_community.embeddings import HuggingFaceEmbeddings  # ìƒˆë¡œìš´ ì„í¬íŠ¸ ê²½ë¡œ
import codecs

def save_metrics(df, cfg, model_name, output_dir):
    # ë°ì´í„° ì „ì²˜ë¦¬
    print("\nData preprocessing stats before:")
    print(f"Total samples: {len(df)}")
    print(f"Missing values:\n{df.isna().sum()}")
    print(f"Duplicates: {df.duplicated().sum()}")
    
    # ê°ì • ë¼ë²¨ ì†Œë¬¸ìë¡œ ë³€í™˜
    df['emotion'] = df['emotion'].str.lower()
    df[f'predicted_emotion_{model_name}'] = df[f'predicted_emotion_{model_name}'].str.lower()
    
    # ê²°ì¸¡ì¹˜ì™€ ì¤‘ë³µ ì œê±°
    df = df.dropna(subset=['emotion', f'predicted_emotion_{model_name}'])
    df = df.drop_duplicates()
    
    print("\nData preprocessing stats after:")
    print(f"Total samples: {len(df)}")
    print(f"Missing values:\n{df.isna().sum()}")
    print(f"Duplicates: {df.duplicated().sum()}")
    
    # NaN ê°’ì„ default_emotionìœ¼ë¡œ ì±„ìš°ê¸°
    default_emotion = cfg.data.default_emotion
    
    # configì—ì„œ ì •ì˜ëœ ë¼ë²¨ ëª©ë¡ ì‚¬ìš©
    labels = cfg.data.datasets[cfg.data.name].labels
    
    # ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    mapping_dict = {
        'unknown': default_emotion,
        'indifference': default_emotion,
        'confusion': default_emotion,
    }
    
    # ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ ì¶”ì¶œí•˜ê³  ë¬¸ìì—´ë¡œ ë³€í™˜
    y_true = df['emotion'].fillna(default_emotion).astype(str)
    y_pred = df[f'predicted_emotion_{model_name}'].fillna(default_emotion).astype(str)
    
    # ì˜ˆì¸¡ ë¼ë²¨ì„ configì˜ ë¼ë²¨ë¡œ ë§¤í•‘
    y_pred = y_pred.map(lambda x: mapping_dict.get(x, x) if x not in labels else x)
    
    print(f"\nLabels information:")
    print(f"Config labels: {labels}")
    print(f"Unique values in true labels: {y_true.unique()}")
    print(f"Unique values in predicted labels after mapping: {y_pred.unique()}")
    
    # ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„± (zero_division=0 ì¶”ê°€)
    report_dict = classification_report(y_true, y_pred, 
                                      labels=labels,
                                      output_dict=True, 
                                      zero_division=0)
    report_str = classification_report(y_true, y_pred, 
                                     labels=labels,
                                     zero_division=0)
    
    # ë¶„ë¥˜ ë³´ê³ ì„œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    with open(output_dir / f'classification_report_{model_name}.txt', "w") as file:
        file.write(report_str)
    
    # Classification Reportë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CSVë¡œ ì €ì¥
    report_df = pd.DataFrame(report_dict).transpose()
    report_df.to_csv(output_dir / f'classification_report_{model_name}.csv')
    
    # í˜¼ë™ í–‰ë ¬ ê³„ì‚° (ëª…ì‹œì ìœ¼ë¡œ ë¼ë²¨ ì§€ì •)
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_normalized = confusion_matrix(y_true, y_pred, labels=labels, normalize='true')
    
    # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ê²½ë¡œ ì„¤ì •
    
    metrics_path = output_dir / f'metrics_{model_name}.json'
    
    # ë©”íŠ¸ë¦­ ê²°ê³¼ ì €ì¥
    metrics_result = {
        "overall": {
            "accuracy": report_dict.get('accuracy', 0.0),
            "macro_avg": report_dict.get('macro avg', {}),
            "weighted_avg": report_dict.get('weighted avg', {})
        },
        "per_class": {
            label: report_dict[label] for label in report_dict.keys()
            if label not in ['accuracy', 'macro avg', 'weighted avg']
        }
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(metrics_path, 'w') as f:
        json.dump(metrics_result, f, indent=2)
        
    # ë‘ ê°œì˜ subplotìœ¼ë¡œ í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))
    
    # ì›ë³¸ í˜¼ë™ í–‰ë ¬
    sns.heatmap(cm, 
                annot=True, 
                fmt='d', 
                cmap='Blues',
                xticklabels=labels,
                yticklabels=labels,
                ax=ax1)
    ax1.set_title(f'Confusion Matrix - {model_name}')
    ax1.set_ylabel('True Label')
    ax1.set_xlabel('Predicted Label')
    
    # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬
    sns.heatmap(cm_normalized, 
                annot=True, 
                fmt='.2f', 
                cmap='Blues',
                xticklabels=labels,
                yticklabels=labels,
                ax=ax2)
    ax2.set_title(f'Normalized Confusion Matrix - {model_name}')
    ax2.set_ylabel('True Label')
    ax2.set_xlabel('Predicted Label')
    
    # ë‘ subplot ëª¨ë‘ì— ëŒ€í•´ ë¼ë²¨ íšŒì „ ë° ì •ë ¬ ì¡°ì •
    for ax in [ax1, ax2]:
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)
    
    # ê·¸ë˜í”„ê°€ ì˜ë¦¬ì§€ ì•Šë„ë¡ ë ˆì´ì•„ì›ƒ ì¡°ì •
    plt.tight_layout()
    
    # ë” ë†’ì€ í•´ìƒë„ë¡œ ì €ì¥
    plt.savefig(output_dir / f'confusion_matrix_{model_name}.png', 
                bbox_inches='tight',
                dpi=300)
    plt.close()
    
    return metrics_result, cm

def map_unknown_emotion(emotion: str, labels: list, mapping_dict: dict = None, cfg: dict = None) -> str:
    """ì•Œ ìˆ˜ ì—†ëŠ” ê°ì •ì„ ë§¤í•‘"""
    if mapping_dict is None:
        mapping_dict = {
            'happy': 'joy',
            'happiness': 'joy',
            'anxious': 'fear',
            'anxiety': 'fear',
            'worried': 'fear',
            'mad': 'anger',
            'frustrated': 'anger',
            'depressed': 'sadness',
            'disappointed': 'sadness',
            'disgusted': 'disgust',
            'ashamed': 'shame',
            'embarrassed': 'shame',
            'regret': 'guilt',
            'remorse': 'guilt',
            'unknown': 'sadness'
        }
    
    mapped_emotion = mapping_dict.get(emotion.lower(), cfg.data.default_emotion)
    print(f"Mapping emotion: {emotion} -> {mapped_emotion}")  # ë§¤í•‘ ë¡œê·¸ ì¶”ê°€
    return mapped_emotion

def retry_emotion_prediction(text: str, labels: list, client, model: str, temperature: float, seed: int) -> dict:
    """ê°ì • ì¬ì˜ˆì¸¡ ì‹œë„"""
    # ë” ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ì™€ ì œì•½ì¡°ê±´ ì¶”ê°€
    tools = [{
        "type": "function",
        "function": {
            "name": "search",
            "description": f"Strictly classify the emotion as one of these ONLY: {', '.join(labels)}",
            "parameters": {
                "properties": {
                    "emotion": {
                        "type": "string",
                        "enum": labels,  # ê°€ëŠ¥í•œ ê°’ì„ ëª…ì‹œì ìœ¼ë¡œ ì œí•œ
                        "description": f"Must be one of: {', '.join(labels)}"
                    },
                    "confidence_score": {
                        "type": "number",
                        "description": "confidence score of the emotion: 0.0-1.0"
                    },
                    "explanation": {
                        "type": "string",
                        "description": "explain why the emotion is decided"
                    },
                },
                "required": ["emotion", "confidence_score", "explanation"]
            }
        }
    }]

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": f"You MUST classify the emotion as one of these ONLY: {', '.join(labels)}. Do not use any other emotion words."},
                {"role": "user", "content": text}
            ],
            tools=tools,
            seed=seed,
            temperature=temperature,
            timeout=10
        )
        
        if response.choices[0].message.tool_calls:
            tool_call = response.choices[0].message.tool_calls[0]
            return json.loads(tool_call.function.arguments)
    except Exception as e:
        print(f"Error in retry prediction: {e}")
    
    return {"emotion": "sadness", "confidence_score": 0.5, "explanation": "Failed to get valid prediction"}

def load_data(cfg):
    dataset_cfg = cfg.data.datasets[cfg.data.name]
    labels = dataset_cfg.labels
    df = pd.read_csv(
        dataset_cfg.path,
        sep=dataset_cfg.separator,
        on_bad_lines='skip',
        encoding='utf-8',
        engine='python',
        quoting=3,
        dtype=str
    )

    required_columns = dataset_cfg.required_columns
    
    if not all(col in df.columns for col in required_columns):
        df.columns = [col.split(dataset_cfg.separator)[0] for col in df.columns]
    
    df = df[required_columns]
    df.rename(columns={'SIT': 'text', 'EMOT': 'emotion'}, inplace=True)
    df['emotion'] = df['emotion'].map(lambda x: labels[int(x)-1] if x.isdigit() else 'undefined')
    
    #timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    def preprocess_text(text):
        if not isinstance(text, str):
            return text
        text = ' '.join(text.split())
        text = text.lower()
        return text
    
    print("\nData preprocessing stats before:")
    print(f"Total samples: {len(df)}")
    print(f"Missing values:\n{df.isna().sum()}")
    print(f"Duplicates: {df.duplicated().sum()}")
    
    # ê²°ì¸¡ì¹˜ ë°ì´í„° ì €ì¥
    missing_data = df[df.isna().any(axis=1)].copy()
    if not missing_data.empty:
        missing_data['removal_reason'] = 'missing_value'
        missing_data['status'] = 'removed'
        missing_data.to_csv(Path(cfg.general.output_path) / f'missing_data.csv', index=True)
        print(f"\nSaved {len(missing_data)} rows with missing values")
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    df_no_missing = df.dropna()
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©
    df_no_missing['text'] = df_no_missing['text'].apply(preprocess_text)
    
    # exclude_phrases ì²˜ë¦¬
    excluded_rows = pd.DataFrame()
    if cfg.data.exclude_phrases:
        exclude_phrases = list(cfg.data.exclude_phrases)
        original_len = len(df_no_missing)
        
        # ì œì™¸ë  í–‰ë“¤ì„ ë¯¸ë¦¬ ì €ì¥
        for phrase in exclude_phrases:
            # í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ exclude_length ë¯¸ë§Œì¸ ê²½ìš°ì—ë§Œ í•„í„°ë§
            matched_rows = df_no_missing[
                (df_no_missing['text'].str.contains(phrase, case=False, na=False)) & 
                (df_no_missing['text'].str.len() < cfg.data.exclude_length)
            ].copy()
            
            if not matched_rows.empty:
                matched_rows['removal_reason'] = f'contains_phrase: {phrase}'
                matched_rows['status'] = 'removed'
                excluded_rows = pd.concat([excluded_rows, matched_rows])
        
        # ì œì™¸ êµ¬ë¬¸ í¬í•¨ëœ í–‰ ì œê±° (ê¸¸ì´ ì¡°ê±´ í¬í•¨)
        for phrase in exclude_phrases:
            df_no_missing = df_no_missing[
                ~((df_no_missing['text'].str.contains(phrase, case=False, na=False)) & 
                  (df_no_missing['text'].str.len() < cfg.data.exclude_length))
            ]
        
        filtered_len = len(df_no_missing)
        print(f"\nRemoved {original_len - filtered_len} rows containing excluded phrases (length < {cfg.data.exclude_length})")
        
        # ì œì™¸ëœ í–‰ ì €ì¥
        if not excluded_rows.empty:
            excluded_rows.to_csv(Path(cfg.general.output_path) / f'excluded_phrases_data.csv', index=True)
            print(f"Saved {len(excluded_rows)} rows containing excluded phrases")
    
    # ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬
    duplicates_all = df_no_missing[df_no_missing.duplicated(subset=['text'], keep=False)].copy()
    if not duplicates_all.empty:
        # ì¤‘ë³µ ë°ì´í„°ì— ê·¸ë£¹ ID ì¶”ê°€
        duplicates_all['duplicate_group'] = duplicates_all.groupby('text').ngroup()
        
        # ê° ê·¸ë£¹ì˜ ì²« ë²ˆì§¸ í•­ëª©ì€ ìœ ì§€ë˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°ë¨ì„ í‘œì‹œ
        duplicates_all['status'] = 'removed'
        duplicates_all.loc[~duplicates_all.duplicated(subset=['text'], keep='first'), 'status'] = 'kept'
        
        # duplicate_groupìœ¼ë¡œ ì •ë ¬í•˜ê³ , ê°™ì€ ê·¸ë£¹ ë‚´ì—ì„œëŠ” statusë¡œ ì •ë ¬ (keptê°€ ë¨¼ì € ì˜¤ë„ë¡)
        duplicates_all = duplicates_all.sort_values(['duplicate_group', 'status'], 
                                                  ascending=[True, True])
        
        # ì¤‘ë³µ ë°ì´í„° ì €ì¥ (ìœ ì§€ëœ ê²ƒê³¼ ì œê±°ëœ ê²ƒ ëª¨ë‘)
        duplicates_all.to_csv(Path(cfg.general.output_path) / f'duplicate_data.csv', index=True)
        print(f"\nSaved {len(duplicates_all)} rows of duplicate data (including kept and removed)")
        print(f"- Kept: {len(duplicates_all[duplicates_all['status'] == 'kept'])} rows")
        print(f"- Removed: {len(duplicates_all[duplicates_all['status'] == 'removed'])} rows")
    
    # ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ í•­ëª© ìœ ì§€)
    df_final = df_no_missing.drop_duplicates(subset=['text'], keep='first')
    
    # ê°ì • í´ë˜ìŠ¤ë³„ í†µê³„ ê³„ì‚°
    emotion_stats = {}
    for emotion in labels:
        initial_count = len(df[df['emotion'] == emotion])
        excluded_count = len(excluded_rows[excluded_rows['emotion'] == emotion])
        duplicates_removed = len(duplicates_all[(duplicates_all['emotion'] == emotion) & 
                                              (duplicates_all['status'] == 'removed')]) if not duplicates_all.empty else 0
        final_count = len(df_final[df_final['emotion'] == emotion])
        
        emotion_stats[emotion] = {
            'initial_count': initial_count,
            'excluded_count': excluded_count,
            'duplicates_removed': duplicates_removed,
            'final_count': final_count,
            'total_removed': excluded_count + duplicates_removed,
            'removal_percentage': ((initial_count - final_count) / initial_count * 100) if initial_count > 0 else 0
        }
    
    # ì œê±°ëœ ë°ì´í„° í†µê³„ ì €ì¥
    removal_stats = {
        'total_initial_samples': len(df),
        'total_final_samples': len(df_final),
        'removed_missing_values': len(missing_data),
        'removed_excluded_phrases': len(excluded_rows),
        'duplicate_stats': {
            'total_duplicates': len(duplicates_all) if not duplicates_all.empty else 0,
            'kept_duplicates': len(duplicates_all[duplicates_all['status'] == 'kept']) if not duplicates_all.empty else 0,
            'removed_duplicates': len(duplicates_all[duplicates_all['status'] == 'removed']) if not duplicates_all.empty else 0
        },
        'excluded_phrases_list': list(cfg.data.exclude_phrases) if cfg.data.exclude_phrases else [],
        'emotion_class_stats': emotion_stats
    }
    
    with open(Path(cfg.general.output_path) / f'removal_stats.json', 'w', encoding='utf-8') as f:
        json.dump(removal_stats, f, indent=2, ensure_ascii=False)
    
    print("\nData preprocessing stats after:")
    print(f"Total samples: {len(df_final)}")
    print(f"Total removed samples:")
    print(f"- Missing values: {len(missing_data)}")
    print(f"- Excluded phrases: {len(excluded_rows)}")
    print(f"- Duplicates: {len(duplicates_all[duplicates_all['status'] == 'removed']) if not duplicates_all.empty else 0}")
    print("\nEmotion class statistics:")
    for emotion, stats in emotion_stats.items():
        print(f"\n{emotion}:")
        print(f"- Initial count: {stats['initial_count']}")
        print(f"- Excluded: {stats['excluded_count']}")
        print(f"- Duplicates removed: {stats['duplicates_removed']}")
        print(f"- Final count: {stats['final_count']}")
        print(f"- Removal percentage: {stats['removal_percentage']:.2f}%")
    
    return df_final, labels

def clean_json_response(response_text: str) -> str:
    """JSON ì‘ë‹µ ì •ë¦¬"""
    try:
        print("\nRaw response:", repr(response_text))  # ì‹¤ì œ ì‘ë‹µ í™•ì¸
        
        # 1. ì½”ë“œ ë¸”ë¡ ì œê±°
        if "```" in response_text:
            pattern = r"```(?:json)?(.*?)```"
            matches = re.findall(pattern, response_text, re.DOTALL)
            if matches:
                response_text = matches[0]
                print("After code block removal:", repr(response_text))
        
        # 2. JSON ë¸”ë¡ ì°¾ê¸°
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        
        if json_start == -1 or json_end <= json_start:
            raise ValueError("No valid JSON found in response")
            
        json_text = response_text[json_start:json_end]
        print("Extracted JSON:", repr(json_text))
        
        # 3. ëª¨ë“  ê³µë°± ë¬¸ì ì •ë¦¬ (ë” ê°•ë ¥í•œ ë°©ì‹)
        # ë¨¼ì € ëª¨ë“  ê³µë°± ë¬¸ìë¥¼ ì¼ë°˜ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        json_text = re.sub(r'[\n\r\t\s]+', ' ', json_text)
        print("After whitespace cleanup:", repr(json_text))
        
        # 4. JSON íŒŒì‹± ë° ì¬ì§ë ¬í™”
        parsed = json.loads(json_text)
        final_json = json.dumps(parsed, ensure_ascii=False, separators=(',', ':'))
        print("Final JSON:", repr(final_json))
        
        return final_json
        
    except Exception as e:
        print(f"Error in clean_json_response: {e}")
        print(f"Original response: {response_text}")
        raise

def get_model_response(text: str, labels: list, client, model: str, temperature: float, cfg, llm=None, logger=None, rag=None, tools=None) -> dict:
    """ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ì‘ë‹µ ë°©ì‹ ì„ íƒ"""
    try:
        # ë¨¼ì € template ì •ì˜
        if cfg.model.use_template:
            model_key = model.lower()
            template = cfg.prompt.get(model_key)
            if not template:
                if logger:
                    logger.info(f"No specific template found for model {model_key}, using default template")
                template = cfg.prompt.emotion_fewshot
        else:
            template = cfg.prompt.emotion

        # RAG ë˜ëŠ” ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if cfg.model.use_rag and rag:
            similar_examples = rag.get_similar_examples(text, k=cfg.rag.k_examples)
            prompt_template = str(rag.get_rag_prompt(text, similar_examples))
            if logger:
                safe_log(logger, 'debug', f"RAG prompt: {prompt_template}")
        else:
            prompt_template = f"""{template}

Text: {text}

IMPORTANT: Return ONLY a single-line JSON object without any formatting."""

        if logger:
            safe_log(logger, 'debug', f"Raw prompt: {prompt_template}")

        try:
            if cfg.model.type == "ollama":
                response = llm.invoke(prompt_template)
            elif cfg.model.type == "anthropic":
                response = client.messages.create(
                    model=model,
                    max_tokens=1000,
                    messages=[
                        {
                            "role": "user",
                            "content": f"{template}\n\nText: {text}"
                        }
                    ],
                    temperature=temperature
                )
                response = response.content[0].text
            else:
                # OpenAI, Upstage ë“±ì€ chat completion ì‚¬ìš©
                messages = [
                    {"role": "system", "content": template},
                    {"role": "user", "content": text}
                ]
                
                if cfg.model.function_calling and cfg.model.type != "ollama":
                    response = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=temperature,
                        tools=tools,
                        seed=int(cfg.general.seed)
                    )
                    # Function calling ì‘ë‹µ ì²˜ë¦¬
                    if response.choices[0].message.tool_calls:
                        tool_call = response.choices[0].message.tool_calls[0]
                        result = json.loads(tool_call.function.arguments)
                        if logger:
                            logger.debug(f"Function call result: {result}")
                        return result
                else:
                    # ì¼ë°˜ ì‘ë‹µ
                    response = client.chat.completions.create(
                        model=model,
                        messages=messages,
                        temperature=temperature
                    )
                response = response.choices[0].message.content

            if logger:
                safe_log(logger, 'debug', f"Raw response: {response}")

            # JSON íŒŒì‹±
            result = clean_json_response(response)
            if isinstance(result, str):
                result = json.loads(result)

            if logger:
                safe_log(logger, 'info', f"Processed response: {result}")
            return result

        except Exception as e:
            if logger:
                safe_log(logger, 'error', f"Error generating response: {e}")
            raise

    except Exception as e:
        if logger:
            safe_log(logger, 'error', f"Error in get_model_response: {e}")
        raise

def get_prompt(cfg, text: str, labels: list, model_name: str) -> str:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ ë° ìƒì„±"""
    if cfg.model.use_template:
        # ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if 'llama' in model_name.lower():
            template = cfg.prompt.llama
        elif 'qwen' in model_name.lower():
            template = cfg.prompt.qwen
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            template = cfg.prompt.target_prompt
        return template
    else:
        return f"""Analyze the emotional content of the following text and classify it as one of: {', '.join(labels)}

Text: {text}

Provide your analysis in a structured format."""

def save_prompt_response(text: str, prompt, response: dict, output_dir: Path, index: int):
    """í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µì„ ë¡œê·¸ë¡œ ì €ì¥"""
    # output_dir ë‚´ë¶€ì— logs í´ë” ìƒì„±
    log_dir = output_dir / "logs"
    log_dir.mkdir(exist_ok=True)
    
    # í”„ë¡¬í”„íŠ¸ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    if hasattr(prompt, 'partial_variables'):
        # RAG í”„ë¡¬í”„íŠ¸ì˜ ê²½ìš°
        prompt_str = prompt.partial_variables.get('context', str(prompt))
    else:
        # ì¼ë°˜ í”„ë¡¬í”„íŠ¸ì˜ ê²½ìš°
        prompt_str = str(prompt)
    
    log_entry = {
        "index": index,
        "input_text": text,
        "prompt": prompt_str,
        "response": response
    }
    
    log_file = log_dir / "prompt_response_log.jsonl"
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

def format_prompt_for_display(prompt, text):
    """í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…"""
    output = "\n" + "="*50 + "\n"
    output += "ğŸ“ PROMPT DETAILS\n" + "="*50 + "\n\n"
    
    # ì…ë ¥ í…ìŠ¤íŠ¸
    output += "ğŸ“Œ Input Text:\n"
    output += f"{text}\n\n"
    
    # RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
    if hasattr(prompt, 'partial_variables') and 'context' in prompt.partial_variables:
        context = prompt.partial_variables['context']
        
        # ìœ ì‚¬ ì˜ˆì œ ì¶”ì¶œ
        if "Similar examples for reference:" in context:
            output += "ğŸ” Similar Examples:\n"
            examples = context.split("Similar examples for reference:\n")[1].split("\nRemember")[0]
            output += f"{examples}\n"
        
        # ì§€ì¹¨ ì¶”ì¶œ
        if "Remember to:" in context:
            output += "ğŸ“‹ Guidelines:\n"
            guidelines = context.split("Remember to:")[1].split("Format your response")[0]
            output += f"{guidelines}\n"
    
    output += "-"*50 + "\n"
    return output

class SafeStreamHandler(logging.StreamHandler):
    def __init__(self, stream=None):
        super().__init__(stream)
        if sys.platform == 'win32':
            if stream is None:
                self.stream = codecs.getwriter('utf-8')(sys.stdout.buffer)
            elif hasattr(stream, 'buffer'):
                self.stream = codecs.getwriter('utf-8')(stream.buffer)

    def emit(self, record):
        try:
            msg = self.format(record)
            try:
                if sys.platform == 'win32' and hasattr(self.stream, 'buffer'):
                    self.stream.buffer.write(msg.encode('utf-8'))
                    self.stream.buffer.write(self.terminator.encode('utf-8'))
                else:
                    self.stream.write(msg)
                    self.stream.write(self.terminator)
                self.flush()
            except UnicodeEncodeError:
                try:
                    safe_msg = msg.encode('utf-8', errors='replace').decode('utf-8')
                    if sys.platform == 'win32' and hasattr(self.stream, 'buffer'):
                        self.stream.buffer.write(safe_msg.encode('utf-8'))
                        self.stream.buffer.write(self.terminator.encode('utf-8'))
                    else:
                        self.stream.write(safe_msg)
                        self.stream.write(self.terminator)
                    self.flush()
                except:
                    safe_msg = msg.encode('ascii', errors='replace').decode('ascii')
                    sys.stderr.write(safe_msg + self.terminator)
                    self.flush()
        except Exception:
            self.handleError(record)

def setup_logging(cfg, output_dir: Path):
    """ë¡œê¹… ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = output_dir / cfg.general.logging.log_path
    log_dir.mkdir(parents=True, exist_ok=True)

    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    log_file = log_dir / f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger('emotion_analysis')
    logger.setLevel(logging.DEBUG)

    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    logger.handlers.clear()

    # Windows í™˜ê²½ì—ì„œ UTF-8 ì¶œë ¥ì„ ìœ„í•œ ì„¤ì •
    if sys.platform == 'win32':
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')

    try:
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
        log_file = log_dir / f"{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8', mode='w')
        file_handler.setLevel(logging.DEBUG)

        # ì½˜ì†” í•¸ë“¤ëŸ¬ ì„¤ì •
        console_handler = SafeStreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)  # INFO ë ˆë²¨ë¡œ ì œí•œ
        
        # í¬ë§·í„° ì„¤ì •
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        logger.addHandler(console_handler)

    except Exception as e:
        print(f"Error setting up logging: {e}")
        # ê¸°ë³¸ ì½˜ì†” ë¡œê±°ë§Œ ì„¤ì •
        console_handler = SafeStreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    return logger

def safe_log(logger, level: str, message: str):
    """ì•ˆì „í•œ ë¡œê¹… í•¨ìˆ˜"""
    try:
        if not isinstance(message, str):
            message = str(message)
        
        # ASCIIë¡œ ë³€í™˜ (í•œê¸€ê³¼ íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        message = message.encode('ascii', errors='ignore').decode('ascii')
            
        # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
        if len(message) > 1000:
            message = message[:997] + "..."
        
        getattr(logger, level)(message)
            
    except Exception as e:
        print(f"Logging error: {e}")

def get_output_dir_name(model_name: str, cfg) -> str:
    """ì„¤ì •ì— ë”°ë¥¸ ì¶œë ¥ í´ë”ëª… ìƒì„±"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    name_parts = [timestamp, model_name]
    
    # ê° ì„¤ì •ì´ Trueì¼ ë•Œë§Œ ì´ë¦„ì— ì¶”ê°€
    if cfg.model.use_template:
        name_parts.append("template")
    if cfg.model.use_rag:
        name_parts.append("rag")
    if cfg.model.function_calling:
        name_parts.append("function_calling")
    
    return "_".join(name_parts)

def initialize_models(cfg):
    """LLMê³¼ RAG ëª¨ë¸ ì´ˆê¸°í™”"""
    llm = None
    rag = None
    
    if cfg.model.type == "ollama":
        llm = OllamaLLM(
            model=cfg.model.name,
            temperature=cfg.model.temperature
        )
    elif cfg.model.type == "openai":
        base_url = "https://api.openai.com/v1"
        api_key = os.getenv("OPENAI_API_KEY")
        llm = OpenAI(
            base_url=base_url,
            api_key=api_key
        )
    elif cfg.model.type == "upstage":
        base_url = "https://api.upstage.ai/v1/solar"
        api_key = os.getenv("UPSTAGE_API_KEY")
        llm = OpenAI(
            base_url=base_url,
            api_key=api_key
        )
    elif cfg.model.type == "anthropic":
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
        llm = Anthropic(api_key=api_key)
    
    if cfg.model.use_rag:
        rag = EmotionRAG(cfg)
    
    return llm, rag

def sanitize_model_name(model_name: str) -> str:
    """ëª¨ë¸ ì´ë¦„ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ì •ë¦¬"""
    return re.sub(r'[^\w\-\.]', '_', model_name.lower())

@hydra.main(version_base="1.2", config_path='config', config_name='llm')
def main(cfg):
    # ëª¨ë¸ ì´ˆê¸°í™”ë¥¼ ë©”ì¸ ë¡œì§ ì‹œì‘ ì „ì— ìˆ˜í–‰
    llm, rag = initialize_models(cfg)
    
    # ëª¨ë“  config ê°’ë“¤ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    model_type = str(cfg.model.type)
    model = str(cfg.model.name)
    temperature = float(cfg.model.temperature)
    
    # ì•ˆì „í•œ ëª¨ë¸ëª… ìƒì„±
    model_name = sanitize_model_name(model)
    print(f"Using sanitized model name: {model_name}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir_name = get_output_dir_name(model_name, cfg)
    output_dir = Path('outputs') / output_dir_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # cfgì˜ output_path ì—…ë°ì´íŠ¸
    cfg.general.output_path = str(output_dir)
    
    
    # ë°ì´í„° ë¡œë“œ ë° ë¼ë²¨ ì¤€ë¹„
    df_isear, labels = load_data(cfg)
    labels = list(map(str, labels))
    
    # n_samples ì„¤ì •
    n_samples = len(df_isear) if cfg.data.n_samples == -1 else min(cfg.data.n_samples, len(df_isear))
    print(f"Processing {n_samples} samples out of {len(df_isear)} total samples")
    
    # ì²˜ìŒ n_samples ê°œë§Œ ì„ íƒ
    df_isear = df_isear.head(n_samples)
    
    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ë””ë ‰í† ë¦¬ëŠ” ì´ë¯¸ ìƒì„±ë¨)
    output_path = Path(cfg.general.output_path) / f'dataset-{cfg.data.name}_model-{model_name}.csv'

    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    if model_type == "openai":
        base_url = "https://api.openai.com/v1"
        api_key = os.getenv("OPENAI_API_KEY")
        client = OpenAI(
            base_url=base_url,
            api_key=api_key  
        )
    elif model_type == "upstage":
        base_url = "https://api.upstage.ai/v1/solar"
        api_key = os.getenv("UPSTAGE_API_KEY")
        client = OpenAI(
            base_url=base_url,
            api_key=api_key  
        )
    elif model_type == "ollama":
        base_url = "http://localhost:11434/v1"
        api_key = "ollama"
        client = OpenAI(
            base_url=base_url,
            api_key=api_key  
        )
    elif model_type == "anthropic":
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
        client = Anthropic(api_key=api_key)
        print("Anthropic Claude model initialized")
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Function calling ë„êµ¬ ì •ì˜
    tools = [
        {
            "type": "function",
            "function": {
                "name": "search",
                "description": "Predict the emotion of the text",
                "parameters": {
                    "properties": {
                        "emotion": {
                            "type": "string",
                            "enum": list(labels),  # ëª…ì‹œì ìœ¼ë¡œ listë¡œ ë³€í™˜
                            "description": f"Classify the emotion of the text as one of given labels: {', '.join(labels)}"
                        },
                        "confidence_score": {
                            "type": "number",
                            "description": "confidence score of the emotion"
                        },
                        "explanation": {
                            "type": "string",
                            "description": "explain why the emotion is decided"
                        },
                    },
                    "required": ["emotion", "confidence_score", "explanation"],
                    "type": "object"
                }
            }
        }
    ]

    # ì˜ëª»ëœ ê°ì • ì˜ˆì¸¡ í†µê³„ ì´ˆê¸°í™”
    invalid_predictions = {
        "total": 0,
        "invalid_samples": [],
        "by_true_label": {str(label): 0 for label in labels}
    }

    # RAG ì´ˆê¸°í™” (use_ragê°€ trueì¼ ë•Œ)
    rag = None
    if cfg.model.use_rag:
        rag = EmotionRAG(cfg)
        rag.create_index(df_isear)

    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(cfg, output_dir)

    for index, row in tqdm(df_isear.iterrows(), total=len(df_isear)):
        try:
            if rag:
                rag.exclude_index(index)  # í˜„ì¬ ì¸ë±ìŠ¤ ì œì™¸
                
            result = get_model_response(
                str(row.text),
                labels,
                client,
                model,
                temperature,
                cfg,
                llm=llm,
                logger=logger,
                rag=rag,
                tools=tools  # tools ì „ë‹¬
            )
            
            # ì˜ˆì¸¡ëœ ê°ì •ì´ ìœ íš¨í•œì§€ ë¨¼ì € í™•ì¸
            original_emotion = result["emotion"].lower()
            
            # ì˜ëª»ëœ ì˜ˆì¸¡ í™•ì¸ ë° ê¸°ë¡
            if original_emotion not in [l.lower() for l in labels]:
                print(f"\nInvalid prediction found at index {index}")
                print(f"Text: {row.text[:100]}...")
                print(f"True emotion: {row.emotion}")
                print(f"Predicted: {original_emotion}")
                
                # ì˜ëª»ëœ ì˜ˆì¸¡ í†µê³„ ì—…ë°ì´íŠ¸
                invalid_predictions["total"] += 1
                invalid_predictions["by_true_label"][str(row.emotion)] += 1
                
                # ì˜ëª»ëœ ì˜ˆì¸¡ ìƒ˜í”Œ ì €ì¥ (ëª¨ë“  í•„ìš”í•œ í•„ë“œ í¬í•¨)
                invalid_sample = {
                    "index": int(index),
                    "text": str(row.text),
                    "true_emotion": str(row.emotion),
                    "predicted_emotion": str(original_emotion),
                    "confidence": float(result.get("confidence_score", 0.0)),
                    "explanation": str(result.get("explanation", "No explanation"))
                }
                invalid_predictions["invalid_samples"].append(invalid_sample)
                print(f"Total invalid samples so far: {len(invalid_predictions['invalid_samples'])}")
                
                # ì¬ì‹œë„ ë˜ëŠ” ë§¤í•‘ ë¡œì§
                if getattr(cfg.general, 'retry_invalid_predictions', False):
                    print("Attempting retry...")  # ì¬ì‹œë„ ì‹œì‘
                    try:
                        retry_result = retry_emotion_prediction(
                            str(row.text), labels, client, model, temperature, int(cfg.general.seed)
                        )
                        print(f"Retry result: {retry_result}")  # ì¬ì‹œë„ ê²°ê³¼ ì¶œë ¥
                        result["emotion"] = retry_result.get("emotion", "unknown")
                        result["confidence_score"] = float(retry_result.get("confidence_score", 0.0))
                        result["explanation"] = str(retry_result.get("explanation", "No explanation from retry"))
                        
                        # ì—¬ì „íˆ ì˜ëª»ëœ ê²½ìš° ë§¤í•‘
                        if result["emotion"].lower() not in [l.lower() for l in labels]:
                            print(f"Retry emotion still invalid: {result['emotion']}, applying mapping")
                            result["emotion"] = map_unknown_emotion(result["emotion"], labels, cfg=cfg)
                    except Exception as e:
                        print(f"Error in retry for row {index}: {e}")
                        result["emotion"] = map_unknown_emotion(result["emotion"], labels, cfg=cfg)
                else:
                    print("Applying direct mapping")
                    result["emotion"] = map_unknown_emotion(original_emotion, labels, cfg=cfg)

            # ê²°ê³¼ ì €ì¥
            df_isear.at[index, f'predicted_emotion_{model_name}'] = result["emotion"]
            df_isear.at[index, f'confidence_score_{model_name}'] = result["confidence_score"]
            df_isear.at[index, f'explanation_{model_name}'] = result["explanation"]
            
            # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ì—ë„ ì €ì¥
            result_log = f'''
=== Prediction Result (Index: {index}) ===
Text: {row.text}
Ground Truth: {row.emotion}
Predicted Emotion: {result['emotion']}
Confidence Score: {result['confidence_score']}
Explanation: {result['explanation']}
{'='*50}
'''
            logger.info(result_log)  # ë¡œê·¸ íŒŒì¼ì— ì €ì¥
            
            # ì½˜ì†” ì¶œë ¥ìš© (ê¸°ì¡´ printë¬¸ ëŒ€ì²´)
            print(result_log)

            if index % 500 == 0:
                df_isear.to_csv(output_path, index=False)
            
            # ë¡œê¹…
            logger.debug(f"Raw response: {result}")
            logger.info(f"Processed response: {result}")

        except Exception as e:
            print(f"Error processing row {index}: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì €ì¥
            df_isear.at[index, f'predicted_emotion_{model_name}'] = "unknown"
            df_isear.at[index, f'confidence_score_{model_name}'] = 0.0
            df_isear.at[index, f'explanation_{model_name}'] = f"Error: {str(e)}"
            continue

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    df_isear.to_csv(output_path, index=False)
    
    # ì˜ëª»ëœ ì˜ˆì¸¡ í†µê³„ ì €ì¥
    invalid_stats = {
        "total_samples": n_samples,
        "invalid_count": invalid_predictions["total"],
        "invalid_percentage": (invalid_predictions["total"] / n_samples) * 100,
        "by_true_label": invalid_predictions["by_true_label"],
        "invalid_samples": invalid_predictions["invalid_samples"]
    }
    
    print(f"\nInvalid prediction statistics:")
    print(f"Total invalid: {invalid_stats['invalid_count']}")
    print(f"Invalid percentage: {invalid_stats['invalid_percentage']:.2f}%")
    print(f"Number of invalid samples collected: {len(invalid_stats['invalid_samples'])}")
    
    # í†µê³„ ì €ì¥
    save_prediction_stats(df_isear, invalid_stats, output_dir, model_name, labels)
    
    # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì €ì¥
    report, cm = save_metrics(df_isear, cfg, model_name, output_dir)
    
    # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶œë ¥
    print("\nClassification Report Summary:")
    print(f"Accuracy: {report['overall']['accuracy']:.4f}")
    print("\nPer-class metrics:")
    for label, metrics in report['per_class'].items():
        print(f"{label}:")
        print(f"  Precision: {metrics['precision']:.4f}")
        print(f"  Recall: {metrics['recall']:.4f}")
        print(f"  F1-score: {metrics['f1-score']:.4f}")

def save_prediction_stats(df, invalid_stats, output_dir, model_name, labels):
    """ì˜ˆì¸¡ í†µê³„ ì €ì¥"""
    try:
        # í†µê³„ íŒŒì¼ ì €ì¥
        stats_file = output_dir / f'prediction_stats_{model_name}.json'
        with open(stats_file, 'w', encoding='ascii') as f:
            json.dump(invalid_stats, f, indent=2, ensure_ascii=True)
    except Exception as e:
        print(f"Error saving prediction stats: {e}")

def metric():
    cfg = OmegaConf.load("config/llm.yaml")
    
    # ê²½ë¡œë¥¼ Path ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì˜¬ë°”ë¥¸ ê²½ë¡œ êµ¬ì„±
    base_path = Path("D:/dev/isp_rag_cluster/outputs")
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë””ë ‰í† ë¦¬ í™•ì¸
    output_folders = [f for f in base_path.iterdir() if f.is_dir()]
    if not output_folders:
        raise FileNotFoundError("No output folders found")
    
    # ê°€ì¥ ìµœê·¼ í´ë” ì„ íƒ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
    latest_folder = max(output_folders, key=lambda x: x.stat().st_mtime)
    latest_folder = Path("outputs/20250228_122946_llama3.2/dataset-isear_model-llama3.2") #Path("outputs/20250228_000911/dataset-isear_model-gpt-3.5-turbo")
    # íŒŒì¼ ê²½ë¡œ êµ¬ì„±
    model_name = "llama3.2"  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ëª…
    model_name = latest_folder.name.split("_model-")[1]
    file_name = f"dataset-isear_model-{model_name}.csv"
    latest_folder = latest_folder.parent
    file_path = latest_folder / file_name
    
    print(f"Looking for file at: {file_path}")  # ë””ë²„ê¹…ìš©
    
    if not file_path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")
    
    # íŒŒì¼ ì½ê¸°
    df_isear = pd.read_csv(file_path)
    
    # ë©”íŠ¸ë¦­ ì €ì¥
    save_metrics(df_isear, cfg, model_name, latest_folder)

if __name__ == "__main__":
    main()
    #metric()
    
